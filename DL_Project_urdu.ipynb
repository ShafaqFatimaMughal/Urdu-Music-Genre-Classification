{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeQ1WGCxv2el"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MuH3lYQAQ8aZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os, json, math, librosa\n",
        "import IPython.display as ipd\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, GlobalAveragePooling2D, ZeroPadding2D, Input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "# from google.colab import drive\n",
        "from statistics import mode\n",
        "# from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTrdzfZBRiiZ"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive') #mounting drive so that we can access the dataset that has been uploaded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmjkQQSqzqe9"
      },
      "source": [
        "\n",
        "\n",
        "# Exploring, Testing and Visualizing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7RAdrcAv_Bc"
      },
      "source": [
        "Loading and exploring the Dataset, testing the audio, making waveplots for the audio, generating spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o06mhgmlQ8ai"
      },
      "outputs": [],
      "source": [
        "MUSIC = '/content/drive/MyDrive/urdu_data/Genres'\n",
        "music_dataset = []  \n",
        "genre_target = []  \n",
        "for root, dirs, files in os.walk(MUSIC):\n",
        "    for name in files:\n",
        "        filename = os.path.join(root, name)\n",
        "        music_dataset.append(filename) #adding song name\n",
        "        genre_target.append(filename.split(\"/\")[5]) #adding song genre    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnIsOHzVQ8al"
      },
      "outputs": [],
      "source": [
        "audio_path = music_dataset[10]\n",
        "print(audio_path)\n",
        "x , sr = librosa.load(audio_path) #loading the 11th file just for trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkMRgY66Q8am"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 5))\n",
        "librosa.display.waveplot(x, sr=sr) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QZA9grUQ8an"
      },
      "outputs": [],
      "source": [
        "X = librosa.stft(x)\n",
        "Xdb = librosa.amplitude_to_db(abs(X))\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
        "plt.title('STFT Spectogram')\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpSIlz3IQ8ap"
      },
      "outputs": [],
      "source": [
        "file_location = audio_path\n",
        "y, sr = librosa.load(file_location)\n",
        "melSpec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "melSpec_dB = librosa.power_to_db(melSpec, ref=np.max)\n",
        "plt.figure(figsize=(10, 5))\n",
        "librosa.display.specshow(melSpec_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
        "plt.colorbar(format='%+1.0f dB')\n",
        "plt.title(\"MelSpectrogram\")\n",
        "plt.tight_layout()\n",
        "plt.show() #display spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyNFM6Stz5Pl"
      },
      "source": [
        "# Loading MFCC of data in JSON\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcUV96bhw25i"
      },
      "source": [
        "Initializing paths and other variables and saving the mfccs of dataset in a json using data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO7LYTFyF-YH"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = '/content/drive/MyDrive/urdu_data/Genres'\n",
        "JSON_PATH = \"data.json\"\n",
        "SAMPLE_RATE = 22050\n",
        "TRACK_DURATION = 30\n",
        "SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHwiz_OOQ8aq"
      },
      "outputs": [],
      "source": [
        "def save_mfcc(dataset_path, json_path, num_mfcc=32, n_fft=2048, hop_length=512, num_segments=5):\n",
        "    data = {\n",
        "        \"mapping\": [],\n",
        "        \"labels\": [],\n",
        "        \"mfcc\": []\n",
        "    }\n",
        "\n",
        "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
        "        \n",
        "        if dirpath is not dataset_path:\n",
        "            semantic_label = dirpath.split(\"/\")[-1]\n",
        "            data[\"mapping\"].append(semantic_label) #stores the genre label\n",
        "            print(\"\\nProcessing: {}\".format(semantic_label))\n",
        "\n",
        "            for f in filenames:\n",
        "\n",
        "                file_path = os.path.join(dirpath, f)\n",
        "                signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n",
        "\n",
        "                #data augmentation\n",
        "                for d in range(num_segments): #generating mfccs for random segments in songs\n",
        "\n",
        "                    if semantic_label in [\"qawwali\", \"ghazal\"]:\n",
        "                        # Starting is 30s after song begins\n",
        "                        rnd = random.randint(SAMPLE_RATE * 30, len(signal)-SAMPLES_PER_TRACK)\n",
        "                        \n",
        "                    elif semantic_label == \"rock\":\n",
        "                        # Staring is 20 seconds after song begins, will not pick a second that is after the last 30sec of song begins\n",
        "                        rnd = random.randint(SAMPLE_RATE * 20, len(signal)-SAMPLES_PER_TRACK)\n",
        "\n",
        "                        \n",
        "                    else:\n",
        "                        # Starting is initial beginning of song (hiphop)\n",
        "                        rnd = random.randint(0, len(signal)-SAMPLES_PER_TRACK)\n",
        "                        \n",
        "                    start = rnd\n",
        "                    finish = rnd + SAMPLES_PER_TRACK #starting second + 30secs\n",
        "\n",
        "                    mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
        "                    mfcc = mfcc.T\n",
        "\n",
        "                    data[\"mfcc\"].append(mfcc.tolist()) #converting to lst as numpy arr not stored by json file\n",
        "                    data[\"labels\"].append(i-1) #stores the genre label index\n",
        "                    print(\"{}, segment:{}\".format(file_path, d+1))\n",
        "    \n",
        "    #creating a json file to save all mfccs and relevant details of each song segment\n",
        "    with open(json_path, \"w\") as fp:\n",
        "        json.dump(data, fp, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRNUfbK-Q8as"
      },
      "outputs": [],
      "source": [
        "# save_mfcc(DATASET_PATH, JSON_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U605XabS0L5f"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Loading JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZsGW0RvQ8at"
      },
      "outputs": [],
      "source": [
        "def load_data(data_path): #open and read json from given path\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    #save mfcc in X and relevant data in y and z\n",
        "    X = np.array(data[\"mfcc\"])\n",
        "    y = np.array(data[\"labels\"])\n",
        "    z = np.array(data['mapping'])\n",
        "    return X, y, z\n",
        "\n",
        "def plot_history(history):\n",
        "    fig, axs = plt.subplots(2)\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "    plt.show()\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "    # load data\n",
        "    X, y, z = load_data(DATASET_PATH_DRIVE)\n",
        "\n",
        "    # create train, validation and test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # add an axis to input sets\n",
        "    X_train = X_train[..., np.newaxis]\n",
        "    X_validation = X_validation[..., np.newaxis]\n",
        "    X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test, z\n",
        "\n",
        "def predict(model, X, y):\n",
        "    X = X[np.newaxis, ...] \n",
        "    prediction = model.predict(X)\n",
        "    predicted_index = np.argmax(prediction, axis=1)\n",
        "    target = z[y] #use actual label index to get genre\n",
        "    predicted = z[predicted_index][0] #use predicted label index to get genre\n",
        "\n",
        "    print(\"Target: {}, Predicted label: {}\".format(target, predicted))\n",
        "    return predicted, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGfA1udHwS4Q"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGjbszOZVsgb"
      },
      "outputs": [],
      "source": [
        "# Original Model\n",
        "def build_model_BN(input_shape): #multilayer perceptron with CNN and batch normalization\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    \n",
        "    model.add(keras.layers.Conv2D(32, (2, 2), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    \n",
        "    model.add(keras.layers.Dense(4, activation='softmax'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGOtLg7jnpwe"
      },
      "outputs": [],
      "source": [
        "#failed attempts at other models\n",
        "def build_model_GlobalPool(input_shape): #multilayer perceptron with CNN but global ave pool\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(keras.layers.Conv2D(64, (1, 1), activation='relu'))\n",
        "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(keras.layers.Conv2D(64, (1, 1), activation='relu'))\n",
        "    model.add(keras.layers.Conv2D(32, (2, 2), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2), strides=(1, 1), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(keras.layers.Conv2D(4, (2, 2), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2), strides=(1, 1), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlLJmGZ80ZDD"
      },
      "source": [
        "# Training model and Plotting results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_brcEzH8KAO"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"./data.json\" #path of json when json created first time\n",
        "DATASET_PATH_DRIVE = \"/content/drive/MyDrive/urdu_data/data.json\" #using json that has been uploaded to drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkeoO1iZ15OJ"
      },
      "outputs": [],
      "source": [
        "X_train, X_validation, X_test, y_train, y_validation, y_test, z = prepare_datasets(0.2, 0.2)\n",
        "input_shape = (X_train.shape[1], X_train.shape[2], 1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiPFDSNV2eXc"
      },
      "source": [
        "Following cell is used to implement our model with batch normalization or the model with global average pooling or with drop out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJgp2_C-Q8av"
      },
      "outputs": [],
      "source": [
        "model = build_model_BN(input_shape)\n",
        "# model = build_model_GlobalPool(input_shape)\n",
        "\n",
        "optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimiser,  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"model-{epoch:02d}-{val_accuracy:0.2f}\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=5000, callbacks=[checkpoint])\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14WFmKVpkGg9"
      },
      "source": [
        "# Transfer Learning with VGG-16\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzzhyi6Q2V4r"
      },
      "source": [
        "Run following cell for VGG model implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC2YXgv-fJw4"
      },
      "outputs": [],
      "source": [
        "# from keras.applications import VGG16\n",
        "t = []\n",
        "g = []\n",
        "for i in X_train:\n",
        "  j = i.reshape((1292, 32))\n",
        "  j = np.repeat(j[:, :, np.newaxis], 3, axis=2)\n",
        "  t.append(j)\n",
        "\n",
        "for i in X_validation:\n",
        "  j = i.reshape((1292, 32))\n",
        "  j = np.repeat(j[:, :, np.newaxis], 3, axis=2)\n",
        "  g.append(j)\n",
        "\n",
        "t = np.array(t)\n",
        "g = np.array(g)\n",
        "model = keras.applications.vgg16.VGG16(include_top=False, weights = \"imagenet\", input_shape=(1292, 32, 3))\n",
        "model.summary()\n",
        "model.trainable = False\n",
        "\n",
        "base_outputs = model.layers[-1].output\n",
        "\n",
        "x = keras.layers.AveragePooling2D(padding=\"same\")(base_outputs)\n",
        "x = keras.layers.Flatten()(x)\n",
        "x = keras.layers.Dense(256, activation='relu')(x)\n",
        "final_outputs = keras.layers.Dense(4, activation = 'softmax')(x)\n",
        "\n",
        "new_model= keras.Model(inputs=model.layers[0].input, outputs=final_outputs)\n",
        "print(new_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJC8RKG1kn8c"
      },
      "outputs": [],
      "source": [
        "optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "new_model.compile(optimizer=optimiser,  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"model-{epoch:02d}-{val_accuracy:0.2f}\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
        "\n",
        "history = new_model.fit(t, y_train, validation_data=(g, y_validation), batch_size=32, epochs=1000, callbacks=[checkpoint])\n",
        "\n",
        "plot_history(history)\n",
        "\n",
        "test_loss, test_acc = new_model.evaluate(X_test, y_test, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBUanqPtr8qx"
      },
      "source": [
        "Saving the model with best accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrpFgUPqqJpR"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('model-2319-0.91')\n",
        "# model.save(\"/content/drive/MyDrive/urdu_data/Best Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYFUmVuu0s_u"
      },
      "source": [
        "# Tesing model on test data and unseen data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7keHFOHsW-s"
      },
      "source": [
        "Load best accuracy model from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIsd2ekLrnlo"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/urdu_data/Best Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2xmG19uwnuR"
      },
      "source": [
        "Making predictions on test data using model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlwPAY-PgUl8"
      },
      "outputs": [],
      "source": [
        "# # tesing and printing predictions on one particular instance\n",
        "# print(\"Predicting on the 106th song segment from test data\")\n",
        "# X_to_predict = X_test[105]\n",
        "# y_to_predict = y_test[105]\n",
        "# predict(model, X_to_predict, y_to_predict)\n",
        "\n",
        "# testing and printing predictions on all of the test data\n",
        "preds = []\n",
        "targets = []\n",
        "print(\"Predicting on all song segments from test data and saving them too...\")\n",
        "for i in range(len(X_test)):\n",
        "  y_pred, y_act = predict(model, X_test[i], y_test[i])\n",
        "  preds.append(y_pred)\n",
        "  targets.append(y_act) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsXCwH5cowO0"
      },
      "source": [
        "Plotting a confusion matrix on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5txQ6qVQ8aw"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(targets, preds)\n",
        "# print(cm)\n",
        "\n",
        "class_label = z #mapping in json had a list of all classes\n",
        "df_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\n",
        "sns.heatmap(df_cm, annot = True, fmt= \"d\", cbar = False)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"Actual Label\")\n",
        "plt.show()\n",
        "count = 0\n",
        "for i in range(len(targets)):\n",
        "  if targets[i] == preds[i]:\n",
        "    count += 1\n",
        "print(count/len(targets) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThHX_3dFwy-L"
      },
      "source": [
        "Testing model on unseen data that is not part of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TmcgKKVsW8j"
      },
      "outputs": [],
      "source": [
        "SAMPLE_RATE = 22050\n",
        "TRACK_DURATION = 30\n",
        "SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION #30secs\n",
        "\n",
        "def new_song_predict(dirc, label, segments): #make prediction on new unseen data \n",
        "    preds = [] #to store predictions of various segments from song\n",
        "    signal, sample_rate = librosa.load(dirc, sr=SAMPLE_RATE)\n",
        "    name = dirc.split('/')[-1]\n",
        "\n",
        "    for i in range(segments): #making 11 predictions of genre by taking 11 random segments\n",
        "        \n",
        "        rnd = random.randint(0, len(signal)-SAMPLES_PER_TRACK)\n",
        "        start = rnd\n",
        "        end = rnd + SAMPLES_PER_TRACK\n",
        "\n",
        "        mfcc_test = librosa.feature.mfcc(signal[start:end], sample_rate, n_mfcc=32, n_fft=2048, hop_length=512)\n",
        "        mfcc_test = mfcc_test.T\n",
        "\n",
        "        mfcc_test = mfcc_test[np.newaxis, ...]\n",
        "        pred = model.predict(mfcc_test)\n",
        "\n",
        "        predic_index = np.argmax(pred, axis=1)\n",
        "\n",
        "        z = ['ghazal', 'hiphop', 'qawwali', 'rock']\n",
        "        pred = str(z[predic_index[0]]) \n",
        "        preds.append(pred)\n",
        "\n",
        "    # print(preds)\n",
        "    pred = mode(preds) #find most frequent prediction of genre in sections\n",
        "\n",
        "    print(\"Track: {}, Target: {}, Predicted label: {}\".format(name, label, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5x0qJhBxCPl"
      },
      "outputs": [],
      "source": [
        "new_song_predict(\"/content/drive/MyDrive/testing/ghazal/Humko Kisi Ke Gham Ne Maara.wav\", \"ghazal\", 11)\n",
        "new_song_predict(\"/content/drive/MyDrive/testing/ghazal/Dil dhadakne ka sabab yaad aaya Noor Jahan.wav\", \"ghazal\", 11)\n",
        "new_song_predict(\"/content/drive/MyDrive/testing/ghazal/Yoon Zindagi Ki Raah Mein.wav\", \"ghazal\", 11)\n",
        "new_song_predict(\"/content/drive/MyDrive/testing/rock/Alif Allah.wav\", \"rock\", 11)\n",
        "new_song_predict(\"/content/drive/MyDrive/testing/rock/Dosti.wav\", \"rock\", 11)\n",
        "new_song_predict(\"/content/drive/MyDrive/testing/rock/Meri Zindagi.wav\", \"rock\", 11)\n",
        "new_song_predict(\"/content/drive/MyDrive/testing/hiphop/Kabhi Kabhi.wav\", \"hiphop\", 11)\n",
        "new_song_predict(\"/content/drive/MyDrive/testing/hiphop/Nazar.wav\", \"hiphop\", 11)\n",
        "new_song_predict(\"/content/drive/MyDrive/testing/hiphop/Stunner.wav\", \"hiphop\", 11)\n",
        "new_song_predict(\"/content/drive/MyDrive/testing/qawwali/Allah Hoo.wav\", \"qawwali\", 11)\n",
        "new_song_predict(\"/content/drive/MyDrive/testing/qawwali/Teri Yaad Ibadat Meri.wav\", \"qawwali\", 11)\n",
        "new_song_predict(\"/content/drive/MyDrive/testing/qawwali/Naara e Haideri.wav\", \"qawwali\", 11)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LmjkQQSqzqe9",
        "OyNFM6Stz5Pl",
        "U605XabS0L5f",
        "DGfA1udHwS4Q",
        "NlLJmGZ80ZDD",
        "14WFmKVpkGg9"
      ],
      "name": "DL_Project_urdu.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "papermill": {
      "duration": 354.017651,
      "end_time": "2021-11-19T04:57:06.997524",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-11-19T04:51:12.979873",
      "version": "2.1.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
